---
title: "Hertie School/SCRIPTS Data Science Workshop Series"
subtitle: "Session 2: Modern data management with R"
author:
- Therese Anders^[Instructor, Hertie School/SCRIPTS, anders@hertie-school.org.]
- Allison Koh^[Teaching Assistant, Hertie School, kohallison3@gmail.com.]
date: January 17, 2020
output: pdf_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
# packages 
if(!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse,
  foreign,
  haven,
  # additions for the workshops 
  gapminder,
  ggalluvial,
  hflights,
  fivethirtyeight
  )
```


Data visualization with `ggplot2`

# Introduction

# Basic Principles of `ggplot2`

In this first part of the workshop, we will go over basic principles of `ggplot2`. We will work with data from the `gapminder` package. First, install `gapminder` and get an overview over the data. The dataset contains information on life expectancy, GDP per capita, and population by country from 1952 to 2007 in increments of 5 years. Let's use the help function to get an overview of the data.

```{r}
# install.packages("gapminder")
library(gapminder)
?gapminder # getting an overview
```

Start by making a copy of the original data in a data frame called `df`. Then use the `str()` function to get an overview over the variable types in the data frame. The dataframe has 1704 observations and 6 variables. 
```{r}
df <- gapminder
str(df)
```

## `ggplot2` package

`ggplot2` was developed by Hadley Wickham based on Leland Wilkinsonâ€™s "grammar of graphics" principles.  According to the "grammar of graphics," you can create each graph from the following components: "a data set, a set of geoms--visual marks that represent data points, and a coordinate
system"  (Wilkinson 2012). You can access the data visualization with `ggplot2` cheat sheet [here](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf).

For most applications, the code to produce a graph in `ggplot2` is roughly structured as follows:

`ggplot(data = , aes(x = , y = , color = , linetype = )) +`

  `geom()` +
  
  `[other graphical parameters, e.g. title, color schemes, background]`
  
* `ggplot()`: Function to initiate a graph in `ggplot2`.
* `data`: Specifies the data frame from which the plot is produced.
* `aes()`: Specifies aesthetic mappings that describe how variables are mapped to the visual properties of the graph. The minimum value that needs to be specified (for univariate data visualization) is the `x` parameter, where `x` specifies the variable to be plotted on the x-axis. Analogously, the `y` parameter specifies the variable to be plotted on the y-axis. Other examples include the `color` parameter, which specifies the variable to be onto different colors, or the `linetype` parameter, which specifies the variable to be mapped onto different line types in case of line graphs.
* `geom()`: Specifies the type of plot to use. There are many different geoms ("geometric objects") to be specified with the `geom()` layer. Some of the most common ones include `geom_point()` for scatterplots, `geom_line()` for line graphs, `geom_boxplot()` for Boxplots, `geom_bar()` for bar plots for discrete data, and `geom_histogram()` for continuous data. 

For an overview of the most important functions and geoms available through `ggplot2`, see the `ggplot2` [cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf).

`ggplot2` is part of the `tidyverse` collection of `R` packages. You can load the entire collection by downloading the `tidyverse` package and loading it using the `library(tidyverse)` command, but for this workshop we will be downloading and calling each package separately.
```{r}
# install.packages("ggplot2")
library(ggplot2)
```

##  Showing data distributions

### Histograms
Histograms graph the distribution of continuous variables. In this first example, we graph the distribution of the life expectancy variable (i.e. `lifeExp`).
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
summary(df$lifeExp)
ggplot(df,
       aes(x = lifeExp)) +
  geom_histogram()
```

**Question 1** Can you make sense of this graph? What is plotted on the x-axis? What is plotted on the y-axis? What specifies the width of each bar? What specifies the height of each bar? 

*A histogram plots the distribution of a variable. The x-axis specifies the values of the variable. The y-axis specifies the number of observations for each value (or group of values) of the variable. The width of the bar specifies which values of the variable are grouped into one bin. The height of the bar specifies the number of observations in each bin.*

**Question 2** Which conclusions do you draw from the histogram above about the distribution of life expectancy in the world?

*The distribution is not normal (i.e. not a bell curve). It is bimodal with a skew to the left. There is a cluster of country-year observations that has a lower life expectancy (approximately 45-60 years), and a cluster of countries with much higher life expectancies (approx 70 years).*

### Adjusting the number of bins
The default number of bins is 30, which means that the entire range of the variable (here 23.60 to 82.60) is split into 30 equally spaced bins. We can change the number of bins manually. Below, we specify 60 bins to approximate a binwidth of 1 year, taking into account the range of the variable `lifeExp`.
```{r,  warning = F, message = F, fig.height = 2, fig.width = 3}
min(df$lifeExp) - max(df$lifeExp) # 60 years
ggplot(df,
       aes(x = lifeExp)) +
  geom_histogram(bins = 60)
```

What if we specified just 5 bins?
```{r,  warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_histogram(bins = 5)
```

## Density plots
We saw that the shape of the distribution is highly influenced by how many bins we specify. If we specify too few bins, we run the risk of masking a lot of variation within the bins. If we specify too many bins, we trade parsimony for detail--which might make it harder to draw conclusions about the overall distribution of the variable of interest from the graph.

Density plots are continuous alternatives to histograms that do not rely on bins. We will cover details about the mechanics behind density plots and their estimation here. Just know that we can interpret the height of the density curve in a similar way that we interpreted the height of the bars in a histogram: The higher the curve, the more observations we have at that specific value of the variable of interest. In this first example, we use the `geom_density()` function to create the density plot.

```{r,  warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_density()
```

If you do not want the density graph to be plotted as a closed polygon, you can instead use the `geom_line()` geometric object function with the `stat = "density"` parameter.
```{r,  warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density")
```

# Controlling the appearance of graphs
The default graphs we have produced so far are not (yet) ready for publication. In particular, they lack informative labels. In addition, we might want to change the appearance of the graph in terms of size, color, linetype, etc.

## Adding title, subtitle, and axes titles
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density")
```

## Adjusting the range of the axes
By default, `ggplot()` adjusted the x-axis to start not at zero but at approximately 23 to reduce the amount of empty space in the plot. We can manually adjust the range of the axes using the `coord_cartesian()` parameter.
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  coord_cartesian(xlim = c(0, 85))
```

**Caution!!** You will sometimes see the command `scale_y_continuous(limits = c(0, 85))` instead of `coord_cartesian(ylim = c(0, 85))`. Note that these are not the same. `coord_cartesian()` only adjusts the range of the axes (it "zooms" in and out), while `scale_y_continuous(limits = c())` subsets the data. For density plots, this does not make a difference. But there are other examples where it alters the actual shape of the graph, rather than just the part of the graph that is visible.

## Changing the color
Any changes to the appearance of the curve itself are made within the argument that specifies the geometric object to be plotted, here `geom_line()`. `R` knows many colors by name; for a great overview see http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf.  
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density", 
            color = "darkblue") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density")
```

We can also use hexadecimal or RGB (red, green, blue) strings to specify colors. There are plenty of online tools to pick colors and extract hexadecimal or RBG strings. One of my favorites is http://www.colorhexa.com. This online tool allows you to specify a color name, hexadecimal, or RGB string, and returns information on color schemes, complementary colors, as well as alternative shades, tints, and tones. It also offers a color blindness simulator.

Suppose, I like the general tone of the darkblue color above, but am worried that it is a bit too dark for my plot. I enter the color "darkblue" into the search field at http://www.colorhexa.com and look for a brighter alternative. Suppose I really like the color displayed in the second tile from the left on the tints scale. I can extract this color's hexadecimal value of `#2727ff` by hovering over the tile of that color.

```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density", 
            color = "#2727ff") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density")
```

We will talk more about color schemes later in the workshop.

## Changing the line type
We can adjust the type of the line via the `linetype` parameter within `geom_line()`. For an overview of line types see  http://sape.inf.usi.ch/quick-reference/ggplot2/linetype.
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density", 
            color = "#2727ff",
            linetype = "dotdash") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density")
```

## Changing the width of the line
We can adjust the width of the line via the `size` parameter within `geom_line()`. Note that the `size` parameter is universal in the way that it controls line width in line plots and point size in scatter plots.
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density", 
            color = "#2727ff",
            linetype = "dotdash",
            size = 2) +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density")
```

## Changing the opacity of the line
We can adjust the opacity of the line via the `alpha` parameter within any geometric object. The `alpha` parameter ranges between zero and one. Adjusting the opacity of the geometric objects is especially important when plotting multiple lines (or objects) in the same graph to reduce overplotting.

```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density", 
            color = "#2727ff",
            linetype = "dotdash",
            size = 2,
            alpha = 0.5) +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density")
```

## Themes
We can alter the appearance of any element in the plot. Below, we change the pre-specified `theme` that `ggplot2` uses to determine the appearance of the plot. Popular options are `theme_bw()` or `theme_minimal()`. For a full list of themes, see https://ggplot2.tidyverse.org/reference/ggtheme.html. We can change all parameters manual using the `theme()` function.

```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density", 
            color = "#2727ff") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw()
```

# Graphing distributions across groups
## Using different colors
Sometimes, we want to compare distributions across different groups in our data set. Suppose, we wanted to assess the distribution of the life expectancy on different continents. We can use the `table()` function to get an overview over the groups in our data.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 5}
table(df$continent)
```

We pass a separate color to the distribution of the `lifeExp` for each continent by specifying the `color` parameter within the aesthetics. Remember, to remove the `color` parameter from the `geom_line()` function. The ability to pass a second variable to the graph with just one aesthetic (here: color) is where the true power of `ggplot2` for data visualization lies. 
```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = lifeExp,
           color = continent)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw()
```

**Question 3** What is the difference between specifying the `color` parameter outside the `aes()` argument versus within the `aes()` argument?

*If the color parameter is specified outside the `aes()` argument, one color is passed all geometric objects of the same type. If the color parameter is specified within the `aes()` argument, different colors are passed to each value of the variable that is passed to the `color` parameter. A separate geometric object will be plotted for value--each in a different color.*

We can adjust the colors used in the plot in a variety of ways. Below, we first use the `scale_color_manual()` function. This will change the colors in both the plot and the legend, based on our manual specification. Within the `scale_color_manual()` argument, we can also specify a name and labels for the legend.
```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = lifeExp,
           color = continent)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  scale_color_manual(values = c("Africa" = "darkorange",
                                "Americas" = "darkblue",
                                "Europe" = "darkgreen",
                                "Asia" = "darkred",
                                "Oceania" = "purple2"),
                     name = "Continent")
```

There are a ton of resources and packages with pre-defined color schemes. The most popular is www.colorbrewer2.org. You can either pick the desired colors manually, or use the `scale_color_brewer()` function in `ggplot2()`.

```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = lifeExp,
           color = continent)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  scale_color_brewer(palette = "BrBG",
                     name = "Continent")
```

Check out the list of color palettes compiled by [Emil Hvitfeldt](https://github.com/EmilHvitfeldt/r-color-palettes/blob/master/type-sorted-palettes.md#sequential-color-palettes). There is even a LaCroix inspired color scheme available using the package `LaCroixColoR`! Another popular option are the color schemes from the [`viridis`](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) package due to their desirable properties with respect to colorblindness and printability. 


## Using different linetypes
Many academic journals will only accept graphs on a gray scale. This means that color will not be enough to differentiate five lines. We can use different line types instead by specifying the `linetype` parameter within the `aes()` argument. This also makes the graph more color blind friendly. Notice below that in order to combine the legends for the `lintype` and `color` aesthetics, we need to pass the same name within the `scale` function.
```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = lifeExp,
           color = continent,
           linetype = continent)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  scale_color_brewer(palette = "Set1",
                     name = "Continent") +
  scale_linetype_discrete(name = "Continent")
```


## Faceting
Another option to graph different groups is to use faceting. This means to plot each value of the variable upon which we facet in a different panel within the same plot. Here, we will use the `facet_wrap()` function. We could also use the `facet_grid()` which allows faceting across more than one variable.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 7}
ggplot(df,
       aes(x = lifeExp)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  facet_wrap(~ continent, nrow = 1)
```

Suppose, we wanted to exclude the plot for Oceania, since it is only comprised of Australia and New Zealand. We can either create a new subsample data frame, or use the `subset()` command directly within `ggplot()`. 

```{r, warning = F, message = F, fig.height = 2.5, fig.width = 7}
ggplot(subset(df, continent != "Oceania"),
       aes(x = lifeExp)) +
  geom_line(stat = "density") +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  facet_wrap(~ continent, nrow = 1)
```

## Boxplots
Another way to show the distribution of variables across groups are boxplots. Boxplots graph different properties of a distribution:

* The borders of the box denote the 25th and 75th percentile.
* The line within the box denotes the median.
* The position of the whiskers (vertical lines) denote the first quartile value minus 1.5 times the interquartile range and the third quartile value plus 1.5 times the interquartile range. We will not go into details here.
* Dots denote outliers (values that lie outside the whiskers), if applicable.

In `ggplot2` we can graph boxplots across multiple variables using the `geom_boxplot()` geometric object. Here, the continuous variable (i.e. `lifeExp`) should be specified as the `y` variable, and the categorical variable (i.e. `continent`) as the `x` variable.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(subset(df),
       aes(x = continent,
           y = lifeExp)) +
  geom_boxplot() +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw()
```
 
 We can flip the axes by using the `coord_flip()` command.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(subset(df),
       aes(x = continent,
           y = lifeExp)) +
  geom_boxplot() +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  coord_flip()
```

The boxplot denotes outlier with points. We could also overlap the boxplot with the original observations using the `geom_point()` aesthetic. This illustrates how many observations are included in each group. Make sure to specify `outlier.shape = NA` within `geom_boxplot()` distinguish between "regular" and outlier observations.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(subset(df),
       aes(x = continent,
           y = lifeExp)) +
  geom_boxplot(outlier.shape = NA) +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  geom_point()
```

This is confusing because there is a lot of overplotting when we add all original observations to the point. We can add jitter to the points. This will add a small random value to each point in either direction and enhance the appearance of the graph. We can control the spread by using the `width` argument. We can also decrease the opacity of the points. The plot below shows that while Oceania has the highest median life expectancy, this value is based on a lot fewer observations as compared to other continents. 

```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(subset(df),
       aes(x = continent,
           y = lifeExp)) +
  geom_boxplot(outlier.shape = NA) +
  labs(title = "Distribution of global life expectancy 1952-2007",
       subtitle = "Data source: Gapminder package",
       x = "Life expenctancy in years",
       y = "Density") +
  theme_bw() +
  geom_point(position = position_jitter(width = 0.15),
             alpha = 0.2)
```

# Saving plots SWITCH OUT CODE TO MOST RECENT PLOT 
We can output your plots to many different format using the `ggsave()` function, including but not limited to `.pdf`, `.jpeg`, `.bmp`, `.tiff`, or `.eps`. Here, we output the graph as a Portable Network Graphics (.png) file. We can specify the size of the output graph as well as the resolution in dots per inch (dpi). If no graph is specified, `ggsave()` will save the last graph that was executed. For us, this is the boxplot in horizontal orientation. If we no not specify the complete file path, the plot will be saved to your working directory.
```{r, warning = F, message = F}
# ggsave("violin_lifeexp_continent.png", width = 6, height = 3, dpi = 400)
```

## Showing relationships in data

### Scatter plots
In their basic form, scatter plots are used to display values of two variables on a Cartesian coordinate system. Below, we inspect the relationship between GDP per capita and life expectancy.
```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = gdpPercap,
           y = lifeExp)) +
  geom_point() +
  labs(title = "Economic wealth and life expectancy",
       x = "GDP per capita",
       y = "Life expectancy") +
  theme_light()
```

The plot above shows a large amount of clustering (and overplotting) on the left side of the plot, while the right side of the plot is sparsely populated with data. This makes it hard to gauge the relationship between the two variables. Below, we make a number of adjustments to the graph to better display the relationship.

```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = gdpPercap,
           y = lifeExp)) +
  geom_point(alpha = 0.4,
             size = 0.5) +
  labs(title = "Economic wealth and life expectancy",
       x = "GDP per capita",
       y = "Life expectancy") +
  theme_light()
```

### Scaling the data
One reason why the plot above is hard to read is rooted in the shape of the distribution of the GDP per capita variable. GDP per capita has a strong right skew. We can correct for this skew and transform the variable to have a more "normal" distribution by taking the natural logarithm. There are multiple ways to do this.

1. Create a new variable [not shown below]
2. Take the natural logarithm within the `aes()` statement when specifying the variable to be displayed.
3. Using (scales)[https://ggplot2.tidyverse.org/reference/scale_continuous.html] to transform the display. Note that the data is transformed before properties such as the range of the axis are determined.

```{r, warning = F, message = F, fig.height = 2, fig.width = 3}
ggplot(df,
       aes(x = gdpPercap)) +
  geom_line(stat = "density") +
  labs(title = "Untransformed distribution")

ggplot(df,
       aes(x = log(gdpPercap))) +
  geom_line(stat = "density") +
  labs(title = "Applying natural log to variable directly")

ggplot(df,
       aes(x = gdpPercap)) +
  geom_line(stat = "density") +
  labs(title = "Transformation using scales") +
  scale_x_continuous(trans = "log")
```

**Question 4** Can you explain the differences between the plot applying the natural log to the variable within the `aes()` function versus using `scale_x_continuous()`.

*Transforming the variable using the natural logarithm within `aes()` causes the x-axis to be displayed in log values. Using `scale_x_continuous()`, the data is transformed in the same way, however, the x-axis is displayed in the original, non-logged version.*


We can use the same principle in bivariate (or multivariate) displays of data. `scale` transformations are extremely helpful, especially when transforming color scales. However, below, I use the transformation on the variable and reflect it in the axis label clarify that it is the relationship between life expectancy and the natural log of GDP per capita that has a strong positive relationship.
```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = log(gdpPercap),
           y = lifeExp)) +
  geom_point(alpha = 0.4,
             size = 0.5) +
  labs(title = "Economic wealth and life expectancy",
       x = "ln GDP per capita",
       y = "Life expectancy") +
  theme_light()
```

### Shape
We can adjust the default symbol used by `ggplot2` to display the points. The parameter is called [`shape`](http://www.sthda.com/english/wiki/ggplot2-point-shapes).
```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = log(gdpPercap),
           y = lifeExp)) +
  geom_point(alpha = 0.4,
             size = 0.5,
             shape = 4) +
  labs(title = "Economic wealth and life expectancy",
       x = "ln GDP per capita",
       y = "Life expectancy") +
  theme_light()
```

We can also have groups of data displayed using different point shapes. Below, we group by continent. We subset the data to just the year 2007 to de-clutter the plot.
```{r, warning = F, message = F, fig.height = 2, fig.width = 5}
ggplot(subset(df, year == 2007),
       aes(x = log(gdpPercap),
           y = lifeExp,
           shape = continent)) +
  geom_point() +
  labs(title = "Economic wealth and life expectancy",
       subtitle = "2007",
       x = "ln GDP per capita",
       y = "Life expectancy") +
  theme_light()
```

### Adding trend lines
The plot above illustrates a strong positive relationship between GDP per capita and life expectancy. We can highlight the direction and strength of the relationship by adding a trend line using the [`geom_smooth()`](https://ggplot2.tidyverse.org/reference/geom_smooth.html) aesthetic. 

The default smoothing method is `loess` for less than 1,000 observations and `gam` (Generalized Additive Models) for observations greater or equal to 1,000. `ggplot2` informs us which smoothing method was used via a message. By default, a 95% confidence interval is added to the trend line. It shows that the negative relationship at higher values of GDP per capita has a much lower precision than the positive relationship we observe for the majority of the observations.
```{r, warning = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = log(gdpPercap),
           y = lifeExp)) +
  geom_point(alpha = 0.4,
             size = 0.5) +
  labs(title = "Economic wealth and life expectancy",
       x = "ln GDP per capita",
       y = "Life expectancy") +
  theme_light() +
  geom_smooth()
```

Alternatively, we can add a linear regression trend line to the data.
```{r, warning = F, fig.height = 2, fig.width = 4}
ggplot(df,
       aes(x = log(gdpPercap),
           y = lifeExp)) +
  geom_point(alpha = 0.4,
             size = 0.5) +
  labs(title = "Economic wealth and life expectancy",
       x = "ln GDP per capita",
       y = "Life expectancy") +
  theme_light() +
  geom_smooth(method = "lm")
```

Finally, we can display separate trendlines for groups of data. For example, suppose we wanted to know how the relationship between GDP per capita and life expectancy varies by continent. We can pass the grouping variable to the `color` (and/or `linetype`) parameter within the `aes()` function. Below, I further reduce the opacity of the points to avoid overplotting. Note that the color grouping is passed to both the `geom_point()` and the `geom_smooth()` aesthetic.
```{r, warning = F, fig.height = 3, fig.width = 5}
ggplot(df,
       aes(x = log(gdpPercap),
           y = lifeExp,
           color = continent)) +
  geom_point(alpha = 0.2,
             size = 1) +
  labs(title = "Economic wealth and life expectancy",
       x = "ln GDP per capita",
       y = "Life expectancy") +
  theme_light() +
  geom_smooth(method = "lm")
```

## Line plots
Line plots are particularly useful for time series data. Below, we will graph the GDP per capita development of China from 1952 to 2007. We select the data for China by using the `subset()` function on the original data frame.
```{r, warning = F, fig.height = 2, fig.width = 4}
ggplot(subset(df, country == "China"),
       aes(x = year,
           y = gdpPercap)) +
  geom_line()
```

We can add points to the line to highlight which observations are available in the underlying data.
```{r, warning = F, fig.height = 2, fig.width = 4}
ggplot(subset(df, country == "China"),
       aes(x = year,
           y = gdpPercap)) +
  geom_line() +
  geom_point()
```

**Practice 2** Create a plot to compare the GDP per capita development of the BRICS countries (Brazil, Russia, India, China, South Africa). Unfortunately, Russia (or the Soviet Union) is not part of the `gapminder` data, so we cannot display it in the plot. Please create a publication-ready graph that can be printed using grayscale. 

```{r, warning = F, fig.height = 2, fig.width = 5, echo = FALSE, message = F}
ggplot(subset(df, country %in% c("Brazil", "Russia", "China", "India", "South Africa")),
       aes(x = year,
           y = gdpPercap,
           linetype = country,
           shape = country)) +
  geom_line(alpha = 0.5) +
  geom_point(alpha = 0.8,
             size = 0.8) +
  theme_light() +
  scale_shape_discrete(name = "Country") +
  scale_linetype_discrete(name = "Country") +
  labs(title = "GDP per capita in BRICS countries",
       x = "Year",
       y = "GDP per capita")
```


```{r, warning = F, fig.height = 2, fig.width = 7, echo = FALSE, message = F}
ggplot(subset(df, country %in% c("Brazil", "Russia", "China", "India", "South Africa")),
       aes(x = year,
           y = gdpPercap)) +
  geom_line(alpha = 0.5) +
  geom_point(alpha = 0.8,
             size = 0.4) +
  theme_light() +
  labs(title = "GDP per capita in BRICS countries",
       x = "Year",
       y = "GDP per capita") +
  facet_wrap(~country, nrow = 1) +
  scale_x_continuous(breaks = seq(1950,2000, 25))

```


### Spaghetti plots
Spaghetti plots are line plots with many lines displayed in a plot. Typically, the lines are very thin and/or have a high level of transparency to show trends in the data. Below, I am graphing the evolution of life expectancy for all countries in the data set. We allow each country to have its own line by using the `group()` parameter inside `aes()`.

```{r, warning = F, fig.height = 2.5, fig.width = 4, message = F}
brics <- c("Brazil", "Russia", "China", "India", "South Africa")
ggplot(df,
       aes(x = year,
           y = lifeExp,
           group = country)) +
  geom_line(alpha = 0.2, 
            size = 0.1) +
  labs(title = "Life expectancy over time",
       x = "Year",
       y = "Life expectancy") +
  theme_light() +
  geom_line(data = subset(df, country %in% "South Africa"),
            color = "red")
```

We could display the temporal trend for select countries using color.

```{r, warning = F, fig.height = 2.5, fig.width = 5, message = F}
ggplot(subset(df, !(country %in% brics)),
       aes(x = year,
           y = lifeExp,
           group = country)) +
  geom_line(alpha = 0.2, 
            size = 0.1) +
  labs(title = "Life expectancy over time",
       x = "Year",
       y = "Life expectancy") +
  theme_light() +
  geom_line(data = subset(df, country %in% brics),
            aes(color = country),
            size = 0.5) +
  scale_color_brewer(name = "",
                     palette = "RdYlBu")
```

In the plot below, I use a log transformation on GDP per capita within the `scale_color_gradient()` function to display GDP per capita values on their original scale but map the color of the lines to the natural log of GDP per capita.
```{r, warning = F, message = F, fig.height = 3, fig.width = 8}
ggplot(df,
       aes(x = year,
           y = lifeExp,
           group = country,
           color = gdpPercap)) +
  
  # Setting up spaghetti plot
  geom_line(alpha = 1,
            size = 0.1) +
  
  # Each continent in a separate panel
  facet_wrap(~continent, nrow = 1) +
  
  # Choosing defalt theme
  theme_light() +
  
  # Adjusting default theme
  theme(panel.background = element_rect(fill = "black"),
        panel.grid = element_line(size = 0.1),
        strip.background = element_rect(fill = "black"),
        strip.text = element_text(color = "white")) +
  
  # Making the colors pop
  scale_color_gradient(low = "#f7ff00",
                      high = "#00f7ff",
                      name = "GDP per capita",
                       trans = "log") +
  
  # Additional appearance adjustments
  labs(x = "Year",
       y = "Life expectancy",
       title = "Global life expectancy") +
  theme(legend.position = "bottom",
        legend.key.width = unit(1.5, "cm")) +
  scale_x_continuous(breaks = seq(1950, 2010, 25))
```

# Why data wrangling in a visualization workshop?
This workshop focuses on data visualization. However, in practice, data visualization is only the last part in a long stream of data gathering, cleaning, wrangling, and analysis. 

![https://d33wubrfki0l68.cloudfront.net/571b056757d68e6df81a3e3853f54d3c76ad6efc/32d37/diagrams/data-science.png](data-science.png)


`ggplot2` is the most powerful when we have "tidy" data. There are three rules for tidy data, based on Hadley Wickham's [R for Data Science](https://r4ds.had.co.nz/tidy-data.html).

1. "Each variable must have its own column."
2. "Each observation must have its own row."
3. "Each value must have its own cell."

If the data are in a tidy format, we can pass separate variables to separate geometric objects (`geoms`) and create layered displays of multiple variables. Thus an important component of creating interesting data visualizations is to get the data to be in the right format. We will also learn a number of new data visualization tools as part of the data wrangling section, including

* Bar charts
* Error bars on plots

RStudio offers a great [Data wrangling cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) you should take a look at.

# Introduction to `dplyr`
`dplyr` does not accept tables or vectors, just data frames (similar to `ggplot2`)! `dplyr` uses a strategy called "Split - Apply - Combine". Some of the key functions include:

* `select()`: Subset columns.
* `filter()`: Subset rows.
* `arrange()`: Reorders rows.
* `mutate()`: Add columns to existing data.
* `summarise()`: Summarizing data set.
* joins: Combine two data frames together

First, lets dowload the package and call it using the `library()` function.
```{r, message = F, warning=F}
# install.packages("dplyr")
library(dplyr)
```

Today, we will be working with a data set from the `hflights` package. The data set contains all flights from the Houston IAH and HOU airports in 2011. Install the package `hflights`, load it into the library, extract the data frame into a new object called `raw` and inspect the data frame.

**NOTE:** The `::` operator specifies that we want to use the *object* `hflights` from the *package* `hflights`. In the case below, this explicit programming is not necessary. However, it is useful when functions or  objects are contained in multiple packages to avoid confusion. A classic example is the `select()` function that is contained in a number of packages besides `dplyr`.
```{r}
# install.packages("hflights")
library(hflights)
raw <- hflights::hflights
str(raw)
```

## Using `select()` and introducing the Piping Operator `%>%`
Using the so-called **piping operator** will make the `R` code faster and more legible, because we are not saving every output in a separate data frame, but passing it on to a new function. First, let's use only a subsample of variables in the data frame, specifically the year of the flight, the airline, as well as the origin airport, the destination, and the distance between the airports.

Notice a couple of things in the code below:

* We can assign the output to a new data set.
* We use the piping operator to connect commands and create a single flow of operations.
* We can use the select function to rename variables.
* Instead of typing each variable, we can select sequences of variables.
* Note that the `everything()` command inside `select()` will select all variables.

```{r}
data <-  raw %>%
  dplyr::select(Month,
                DayOfWeek,
                DepTime,
                ArrTime,
                ArrDelay,
                TailNum,
                Airline = UniqueCarrier, #Renaming the variable
                Time = ActualElapsedTime, #Renaming the variable
                Origin:Cancelled) #Selecting a number of columns. 
names(data)
```

Suppose, we didn't really want to select the `Cancelled` variable. We can use `select()` to drop variables.
```{r}
data <- data %>%
  dplyr::select(-Cancelled)
```

## Introducting `filter()`
There are a number of key operations when manipulating observations (rows).

* `x < y`
* `x <= y`
* `x == y`
* `x != y`
* `x >= y`
* `x > y`
* `x %in% c(a,b,c)` is `TRUE` if `x` is in the vector `c(a, b, c)`.

Suppose, we wanted to filter all the flights that have their destination in the greater Los Angeles area, specifically Los Angeles (LAX), Ontario (ONT), and John Wayne (SNA) airports. Note that based on the `hflights` dataset, there are no flights from the Houston area to Bob Hope (BUR) or Long Beach (LGB) airports.
```{r, warning = F}
airports <- c("LAX", "ONT", "SNA")

la_flights <- data %>%
  filter(Dest %in% airports)
```

**Caution**: The following command does not return the flights to LAX or ONT!
```{r}
head(la_flights)
la_flights_alt <- data %>%
  filter(Dest == c("LAX", "ONT"))
head(la_flights_alt)
```

Why? We are basically returning all values for which the following is `TRUE` (using the correct output of the `la_flights` data frame:

`Dest[1] == LAX`

`Dest[2] == ONT`

`Dest[3] == LAX`

`Dest[4] == ONT` ...


## Introducting `mutate()`
Currently, we have two taxi time variables in our data set: `TaxiIn` and `TaxiOut`. I care about total taxi time, and want to add the two together. Also, people hate sitting in planes while it is not in the air. To see how much time is spent taxiing versus flying, we create a variable which measures the proportion of taxi time of total time of flight.
```{r}
la_flights <- data %>%
  filter(Dest %in% airports) %>%
  mutate(TaxiTotal = TaxiIn + TaxiOut,
         TaxiProp = TaxiTotal/Time)
```

We can the graph the average proportion of taxi time per airline. 
```{r,  warning = F, message = F, fig.height = 2, fig.width = 3}
library(ggplot2)
ggplot(la_flights,
       aes(x = Airline,
           y = TaxiProp)) +
  geom_boxplot()
```

There is only three airlines flying to LA out of Houston. Lets create a new variable with the airline name using the `case_when()` function to make the graph more informative.
```{r,  warning = F, message = F, fig.height = 3, fig.width = 4}
table(la_flights$Airline)
la_flights <- data %>%
  filter(Dest %in% airports) %>%
  mutate(TaxiTotal = TaxiIn + TaxiOut,
         TaxiProp = TaxiTotal/Time,
         AirlineName = case_when(
           Airline == "CO" ~ "Continental Airlines",
           Airline == "MQ" ~ "American Eagle",
           Airline == "WN" ~ "Southwest Airlines"
         ))

ggplot(la_flights,
       aes(x = AirlineName,
           y = TaxiProp)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "Time spent taxiing",
       x = "Taxiing/flight duration",
       y = "") +
  theme_light()
```

## Introducting `summarise()` and `arrange()`
One of the most powerful `dplyr` features is the `summarise()` function, especially in combination with `group_by()`.

First, in a simple example, lets compute the average delay from Houston to Los Angeles by each day of the week. Note that the arrival delay variable is given in minutes. Also, I want to know what standard deviation of the delay is for each day of the weak. Note, that because there are missing values, we need to tell `R` what to do with them.
```{r,  warning = F, message = F, fig.height = 2, fig.width = 3}
la_flights_delay <- la_flights %>%
  group_by(DayOfWeek) %>%
  summarise(av_delay = mean(ArrDelay, na.rm = T),
            sd_delay = sd(ArrDelay, na.rm = T))
```

We can use error bars to show the standard deviation of the delay time for each day of the weak. I add a line to denote no delay using the `geom_hline()` geometric object.
```{r,  warning = F, message = F, fig.height = 2.5, fig.width = 5}
ggplot(la_flights_delay,
       aes(x = DayOfWeek,
           y = av_delay,
           ymin = av_delay - sd_delay,
           ymax = av_delay + sd_delay)) +
  geom_point() +
  geom_errorbar() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  
  # Making the graph prettier
  scale_x_continuous(breaks = seq(1,7)) +
  theme_light() +
  labs(y = "Average delay (with 1SD)",
       x = "Day",
       title = "Arrival delay") +
  coord_flip()
```

Suppose, I wanted to know whether some airlines have on average shorter arrival delays than others. We can add the airline to the `group_by()` function to compute the mean and standard deviation of arrival delay per day and airline. 
```{r,  warning = F, message = F, fig.height = 2.5, fig.width = 6}
la_flights_delay_airline <- la_flights %>%
  group_by(DayOfWeek, AirlineName) %>%
  summarise(av_delay = mean(ArrDelay, na.rm = T),
            sd_delay = sd(ArrDelay, na.rm = T))

# Plotting it
ggplot(la_flights_delay_airline,
       aes(x = DayOfWeek,
           y = av_delay,
           ymin = av_delay - sd_delay,
           ymax = av_delay + sd_delay,
           color = AirlineName)) +
  geom_point() +
  geom_errorbar() +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  
  # Making graph prettier
  theme_light() +
  coord_flip() +
  scale_x_continuous(breaks = seq(1,7))
```

To de-clutter the graph, below, I use the `geom_linerange()` geometric object rather than `geom_errorbar()`. I can use the `position = dodge` command within the `geom_point()` and `geom_linerange()` geometric object to display the values for each airline next to each other, instead on top of each other. Note that I could have used `position = dodge` with `geom_errorbar()` as well; the functionality is essentially the same.
```{r,  warning = F, message = F, fig.height = 2.5, fig.width = 6}
ggplot(la_flights_delay_airline,
       aes(x = DayOfWeek,
           y = av_delay,
           ymin = av_delay - sd_delay,
           ymax = av_delay + sd_delay,
           color = AirlineName)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_linerange(position = position_dodge(width = 0.5),
                 alpha = 0.5) +
  geom_hline(yintercept = 0,
             linetype = "dashed") +
  
  # Making graph prettier
  theme_light() +
  coord_flip() +
  scale_x_continuous(breaks = seq(1,7)) +
  
  # Matching order of legend and graph
  guides(color = guide_legend(reverse = T))
```


## Joins
`dplyr` has powerful tools to merge data frames together. Because we want to focus on data visualization here, I will not go over all possible joints in depth. Please see the [Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) and the [dplyr documentation](https://dplyr.tidyverse.org/reference/join.html) for more details.

Suppose, we have two data frames: `x` and `y`. The basic syntax for data merging with `dplyr` is the following:

`output <- join(A, B, by = "variable")`

We will focus on the following three join functions:

* `left_join()`: Join only those rows from `y` that appear in `x`, retaining all data in `x`. Here, `x` is the "master."
* `right_join()`: Join only those rows from `x` that appear in `y`, retaining all data in `y`. Here, `y` is the "master."
* `full_join()`: Join data from `x` and `y` upon retaining all rows and values. This is the maximum join possible. Neither `x` nor `y` is the "master."


For demonstration purposes, lets create a new data frame that contains the name of the city for each of the Greater Los Angeles Area airports.
```{r}
loc_airport <- data.frame(code = c("LAX", "ONT", "SNA", "BUR"),
                          location = c("Los Angeles", "Ontario", "Santa Ana", "Burbank"))
loc_airport
```


First, we treat the `la_flights` data frame as the master and join it with the data frame containing the airport locations using `left_join()`. If the variable names in both data frames were the same, `dplyr` would automatically join the correct columns. Here, we manually match the column names.
```{r}
la_flights_new <- left_join(la_flights, loc_airport, 
                            by = c("Dest" = "code"))
table(la_flights_new$Dest)
```

Second, lets create a similar result using `right_join()`. Again, `la_flights` is the master data frame.
```{r}
la_flights_new2 <- right_join(loc_airport, la_flights, 
                              by = c("code" = "Dest"))
table(la_flights_new2$code)
```

Finally, for demonstration, we create a third data frame using `full_join()`. Because all observations are retained, this join creates one observation with empty values for the Burbank value in `loc_airport`. For most applications, this would be an undesirable outcome. However, below, we use the fact that all possible values are retained to set up the data for visualization.
```{r}
la_flights_new3 <- full_join(la_flights, loc_airport, 
                            by = c("Dest" = "code"))
table(la_flights_new3$Dest)
la_flights_new3[la_flights_new3$Dest == "BUR",]
```



# Heatmaps
For this example, we will go back to our original `data` tibble that contains the complete set of flight data for the Houston airports in 2011. Suppose we wanted to know, what are the busiest times at each of the two Houston airports, George Bush Intercontinental/Houston Airport (IAH) and William P. Hobby Airport (HOU). We create a new summary data frame that counts the number of departures per hour and day for each of the airports. We display these data using heatmaps.

To do so, we need to create a new variable that codes the hour of departure, using information from the `DepTime` variable. There are more advanced workflows available using the `stringr` and/or `lubridate` packages (both are part of the `tidyverse`). However, because we want to focus on data visualization, I simply divide the departure time by 100 and then use the `floor()` function to extract the hour of departure.
```{r,  warning = F, message = F, fig.height = 2, fig.width = 5}
departures <- data %>%
  
  dplyr::mutate(hour = floor(DepTime/100))  %>%
  
  group_by(DayOfWeek,
           hour) %>%
  summarise(count = n())

ggplot(departures,
       aes(x = hour,
           y = DayOfWeek,
           fill = count)) +
  geom_tile()
```

There are a number of ways to improve the plot. We will go through them step by step. Their is a weird observation in hour 24 that should not be there. The hour has to be either 23 or 0. Lets re-code this observation to an hour value of 0 using the `replace()` function from `dplyr`.
```{r, warning = F, message = F, fig.height = 2, fig.width = 6}
test <- data %>%
  filter(!is.na(DepTime)) %>%
  mutate(hour = floor(DepTime/100)) %>%
  filter(hour >= 24)
test

# Recoding
departures <- data %>%
  filter(!is.na(DepTime)) %>%
  mutate(hour = floor(DepTime/100))  %>%
  mutate(hour = replace(hour, hour >= 24, 0)) %>%
  group_by(Origin,
           DayOfWeek,
           hour) %>%
  summarise(count = n())

ggplot(departures,
       aes(x = hour,
           y = DayOfWeek,
           fill = count)) +
  geom_tile() 
```

There are a number of possible observations that do not have value in the data frame, in particular in the early morning ours. For this application, we can assume that that these observations are not actually missing, but that there are no flights during these time slots. 

Therefore, we create a data frame with all possible combinations of the variable values for day of the week and hour using `expand.grid()`, and use the `full_join()` function to create a new data frame. Similar to the application above, this procedure will result in missing values. We again use the `replace` function to re-code these missing values to zero.
```{r, warning = F, message = F, fig.height = 2, fig.width = 6}
# Empty data frame
combo <- expand.grid(DayOfWeek = seq(1, 7),
                     hour = seq(0,23),
                     Origin = c("IAH", "HOU"))

# Merging
departures <- data %>%
  filter(!is.na(DepTime)) %>%
  mutate(hour = floor(DepTime/100))  %>%
  mutate(hour = replace(hour, hour >= 24, 0)) %>%
  group_by(Origin,
           DayOfWeek,
           hour) %>%
  summarise(count = n()) %>%
  
  # joining empty data frame
  full_join(combo) %>%
  
  # replacing missing values with zero
  mutate(count = replace(count, is.na(count), 0))

# visualizing it
ggplot(departures,
       aes(x = hour,
           y = DayOfWeek,
           fill = count)) +
  geom_tile() 
```

Now, lets change the appearance of the graph. Below, we use color scales from the `viridis` package.
```{r, warning = F, message = F, fig.height = 5, fig.width = 7}
# install.packages("viridis")
library(viridis)


# visualizing it
ggplot(departures,
       aes(x = hour,
           y = DayOfWeek,
           fill = count)) +
  
  geom_tile(color = "white") +
  scale_fill_viridis(name = "Flights") +
  scale_x_continuous(breaks = seq(0,23)) +
  scale_y_continuous(breaks = seq(1,7),
                     labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")) +

  coord_flip() +
  labs(x = "Hour",
       y = "",
       title = "Departures from Houston airports") +
  
  # Changing appearance of the plot
  theme_light() +
  theme(panel.grid = element_blank(),
        legend.position = "bottom",
        legend.key.width = unit(1.5, "cm"),
        panel.border=element_blank()) +
  
  # Making the space equal by fixing aspect ratio 
  # Reducing space
  coord_fixed(expand = c(0,0))

table(departures$DayOfWeek)
```

# Alluvial diagrams
What are the flows between the two Houston airports and the ten most common destinations? We can visualize the combination of origin airport (IAH versus HOU) and the destination airport using alluvial diagrams. Below, we use the `ggalluvial` package, which contains the `geom_alluvium()` geometric object. 

First, we create a frequency table for all observed combinations of origin and destination airport for the ten most common destinations using `group_by()` and `slice()`.
```{r, warning = F, message = F, fig.height = 4, fig.width = 6}
dest_top10 <- data %>%
  group_by(Dest) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice(1:10)

flows <- data %>%
  filter(Dest %in% dest_top10$Dest) %>%
  group_by(Origin, 
           Dest,
           Airline) %>%
  summarise(count = n())

# install.packages("ggalluvial")
library(ggalluvial)
ggplot(flows,
       aes(y = count,
           axis1 = Origin, 
           axis2 = Dest)) +
  geom_alluvium()
```

We can use fill to make the graph more interesting.
```{r, warning = F, message = F, fig.height = 4, fig.width = 6}
ggplot(flows,
       aes(y = count,
           axis1 = Origin, 
           axis2 = Dest)) +
  geom_alluvium(aes(fill = Origin))
```

We can add labels to illustrate the destination airport. We also add the `geom_stratum()` geometric object to clarify the grouping.
```{r, warning = F, message = F, fig.height = 4, fig.width = 6}
ggplot(flows,
       aes(y = count,
           axis1 = Origin, 
           axis2 = Dest)) +
  geom_alluvium(aes(fill = Origin)) +
  geom_stratum(width = 1/12, fill = "black", color = "grey") +
  geom_label(stat = "stratum", label.strata = TRUE)
```

The plot above looks nice, but the distinction by fill is not necessarily needed. We could instead display an additional variable, for example the airline. 

```{r, warning = F, message = F, fig.height = 4, fig.width = 6}
ggplot(flows,
       aes(y = count,
           axis1 = Origin, 
           axis2 = Dest)) +
  geom_alluvium(aes(fill = Airline)) +
  geom_stratum(width = 1/12, fill = "black", color = "grey") +
  geom_label(stat = "stratum", label.strata = TRUE)
```

The plot above is hardly legible because there are too many airlines displayed. Lets only label the most common ones. First, we create a quick barplot to check who are the most common carriers on the top ten routes. Then, create a new variable coding only the most common, i.e. "Continental" (CO), "Southwest" (WN), and "Other" using `case_when()`.
```{r, warning = F, message = F, fig.height = 2, fig.width = 4}
ggplot(flows,
       aes(x = Airline,
           y = count)) +
  geom_bar(stat = "identity")

# Creating new indicator
flows <- flows %>%
  mutate(Airline_reduced = case_when(
    Airline == "CO" ~ "Continental",
    Airline == "WN" ~ "Southwest",
    T ~ "Other"
  ) %>% factor(levels = c('Continental', 'Southwest', 'Other')))
table(flows$Airline_reduced)
```

Now, we can re-plot the alluvial diagram.
```{r, warning = F, message = F, fig.height = 4, fig.width = 6}
ggplot(flows,
       aes(y = count,
           axis1 = Origin, 
           axis2 = Dest)) +
  geom_alluvium(aes(fill = Airline_reduced)) +
  geom_stratum(width = 1/12, fill = "black", color = "grey") +
  geom_label(stat = "stratum", label.strata = TRUE) +
  scale_fill_manual(name = "Airline",
                    values = c("Continental" = "blue",
                               "Southwest" = "darkorange",
                               "Other" = "grey")) +
  theme_light() +
  labs(title = "Flights from Houston 2011",
       x = "",
       y = "Number of flights") +
  theme(axis.text.x = element_blank())
```

# Primer on `tidyr`
Another important task in data management is data re-shaping. Often, data does not come in the format that we need for data merging, data visualization, statistical analysis, or vectorized programming.

The `tidyr` package offers two main functions for data re-shaping:

* `gather()`: Shaping data from wide to long.
* `spread()`: Shaping data from long to wide.

## Wide versus long data
For **wide** data formats, each unit's responses are in a single row. For example:

| Country | Area | Pop1990 | Pop1991 |
|---------|------|---------|---------|
| A       | 300  | 56      | 58      |
| B       | 150  | 40      | 45      |

For **long** data formats, each row denotes the observation of a unit at a given point in time. For example:

| Country | Year | Area | Pop |
|---------|------|------|-----|
| A       | 1990 | 300  | 56  |
| A       | 1991 | 300  | 58  |
| B       | 1990 | 150  | 40  |
| B       | 1991 | 150  | 45  |

## `gather()`
We use the `gather()` function to reshape data from wide to long. In general, the syntax of the data is as follows:

`new_df <- gather(old_df, key, value, columns to gather)`,

where `key` specifies the column name of the variable capturing variable labels (i.e. the original names of the columns to be re-shaped) and `value` the column values to be stored.

Below, we use the `murder_2015_final` data set from the `fivethirtyeight` package. The data contains number of murders in 83 U.S. cities. The dataset contains a column `murder_2014` and a column `murder_2015`. For tidy data, we want one observation per row and one variable per column. The data is untidy because the two columns confuse the variables `murder` and `year. 

Below, we use `gather()` to tidy the data. For illustration we drop the variable change to show how to re-create it.

```{r}
# install.packages("tidyr")
library(tidyr)

# install.packages("fivethirtyeight")
library(fivethirtyeight)
murder <- fivethirtyeight::murder_2015_final
head(murder)

# using gather to re-shape 
murder_tidy <- murder %>%
  dplyr::select(-change) %>%
  gather(murders_year, value, murders_2014:murders_2015) 
head(murder_tidy)
```

We can use the `separate()` function from the `tidyr` package to turn the column `murders_year` into two separate columns and then drop the `murder` column.

**NOTE: An alternative way would be to use regular expressions and the `stringr` package to extract the year from the `murders_year` column.**

```{r}
murder_tidier <- murder %>%
  dplyr::select(-change) %>%
  gather(murders_year, murders, murders_2014:murders_2015) %>%
  separate(murders_year, c("trash", "year"), sep = "_") %>%
  dplyr::select(-trash)
head(murder_tidier)
```

### Dataviz: Barplots
Suppose we wanted to know what was the city in Florida with the overall highest number of murders. Now that the data is tidy, we can create a grouped bar plot, showing the 2014 and 2015 values with different fill colors.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 5}
ggplot(subset(murder_tidier, state == "Florida"),
       aes(x = city,
           y = murders,
           fill = year)) +
  geom_bar(stat = "identity") +
  coord_flip()
```

**Question** Is the plot above showing what we want? How would you improve it?

*The plot above is confusing! It is adding together the murders for 2014 and 2015, and differences are hard to gauge*

We can use `position = "dodge" within the `geom_bar()` statement place the bars for 2014 and 2015 next to each other and group them by city.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 5}
ggplot(subset(murder_tidier, state == "Florida"),
       aes(x = city,
           y = murders,
           fill = year)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(x = "",
       y = "Murders",
       title = "Murders in Florida")
```

### Creating the first difference
Below, we create a variable that captures the first difference of murders between 2014 and 2015 for each city using the `lag()` function. in combination with `group_by()`. Make sure your data is ordered in the right way using `arrange()` before taking the lagged value $t - 1$ and subtracting it from the value at $t$.

Note, that we need to change the `year` variable from character to numeric to make the code work.
```{r}
str(murder_tidier)
murder_change <- murder_tidier %>%
  mutate(year = as.numeric(year)) %>%
  group_by(city) %>%
  arrange(year) %>%
  
  # Creating variable for first difference
  mutate(diff = murders - lag(murders),
         
         # Creating indicator for negative differences
         diff_neg = ifelse(diff < 0, 1, 0))
```

We can visualize this difference for cities in Florida using a bar plot.
```{r, warning = F, message = F, fig.height = 2, fig.width = 5}
summary(murder_change$diff)
ggplot(subset(murder_change, !is.na(diff) & state == "Florida"),
       aes(x = city,
           y = diff)) +
  geom_bar(stat='identity') +
  coord_flip()
```

The plot can be improved by ordering the bars based on the difference in murder rate.
```{r, warning = F, message = F, fig.height = 2, fig.width = 5}
ggplot(subset(murder_change, !is.na(diff) & state == "Florida"),
       aes(x = reorder(city, diff),
           y = diff)) +
  geom_bar(stat='identity') +
  coord_flip() +
  theme_light() +
  labs(x = "Difference in murders",
       y = "",
       title = "Change in murders 2014-2015")
```

Suppose, I wanted to know whether murders appear to increase in cities that already had a large number of murders (i.e. above average in 2014), or whether it is "low murder rate" cities experiencing more homicides. We could plot the change in the number of murders for all cities in the data frame. This will create a very large bar plot that is hard to read without appropriately grouping the data. 

Below, create a new data frame from `murder_change`, called `murder_change_av`, that adds a dummy variable coded 1 for observations that had above average murder rates in 2014 (taking into account only the year 2014), and 0 otherwise. Note that in the code below, we are not taking the population size of the cities into account, and plot just the absolute values.
```{r, warning = F, message = F, fig.height = 10, fig.width = 8}
murder_change_av <- murder_change %>%
  ungroup() %>%
  mutate(aboveav2014 = ifelse(murders >= mean(murders[year == 2014]), 1, 0)) %>%
  ungroup()

ggplot(subset(murder_change_av, !is.na(diff)),
       aes(x = reorder(city, diff),
           y = diff,
           fill = factor(aboveav2014))) +
  geom_bar(stat = 'identity') +
  coord_flip() +
  theme(axis.text.x = element_text(size = 1),
        legend.position = "none") +
  theme_light() +
  labs(title = "Drivers of increase in murder rates",
       y = "Change from 2014 to 2015",
       x = "") +
  scale_fill_manual(name = "Above average\nin 2014",
                    values = c("0" = "grey",
                               "1" = "orange"),
                    labels = c("No", "Yes"))
  
```

## `spread()`
Suppose we wanted to revert our operation (or generall shape data from a long to a wide format), we can use `tidyr`'s `spread()` function. The syntax is similar to `gather()`.

`new_df <- spread(old_df, key, value)`,

where `key` refers to the colum which contains the values that are to be converted to column names and `value` specifies the column that contains the value which is to be stored in the newly created columns.

For illustration purposes, we first remove the `diff` column below, because it leads to `NA` values when performing the `spread()` operation.
```{r}
murders_untidy <- murder_change %>%
  dplyr::select(-diff) %>%
  spread(year, murders)
head(murders_untidy)
```



### An additional note on barplots
Lets go back to the `hflights` data. Suppose we wanted to know which airline operates the most flights out of either Houston airport. Here, we will be using the operator `n()` to tell `dplyr` to count all the observations for the groups specified in `group_by()`. After computing the result, I would like to arrange the output from highest number of flights to lowest number.
```{r}
carriers <- data %>%
  group_by(Airline) %>%
  summarise(NoFlights = n()) %>%
  arrange(desc(NoFlights))
```

We can display the result graphically using the `geom_bar()` geometric object. Note the following details on the usage of `geom_bar()` from the `ggplot2` package documentation below.

*"The heights of the bars commonly represent one of two things: either a count of cases in each group, or the values in a column of the data frame. By default, geom_bar uses stat="bin". This makes the height of each bar equal to the number of cases in each group, and it is incompatible with mapping values to the y aesthetic. If you want the heights of the bars to represent values in the data, use stat="identity" and map a value to the y aesthetic."* (https://www.rdocumentation.org/packages/ggplot2/versions/1.0.1/topics/geom_bar)

Thus, the creating the count variables using `group_by()` and `summarise()` is not absolutely necessary. However, for more complicated groupings of data, I highly recommend creating a separate data frame and "hard code" groupings of interest before graphing.
```{r, warning = F, message = F, fig.height = 2, fig.width = 6}
# Using default geom_bar(stat = "bin#) on the original data
ggplot(data,
       aes(x = Airline)) +
  geom_bar()

# Using geom_bar(stat = "identity") on grouped data
ggplot(carriers,
       aes(x = Airline,
           y = NoFlights)) +
  geom_bar(stat = "identity")
```


# Advanced bar plots and  `lubridate`
`lubridate` is another package in the `tidyverse` family that makes working with dates easier. Please refer to https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html for more details on the package. 

Below, we use conflict event data from the Armed Conflict Location & Event Data (ACLED) project. Please retrieve the ACLED event data from https://www.acleddata.com/data/ (Pakistan, selecting Battles and Violence against civilians). We use the `readr` package to read the .csv file downloaded from ACLED into `R`.
```{r, warning = F, message = F, fig.height = 10, fig.width = 8}
library(lubridate)

# loading the ACLED data
library(readr)
acled <- read_csv("1900-01-01-2019-05-03-Pakistan.csv") 
acled_new <- acled %>%
  mutate(date = dmy(event_date)) %>%
  group_by(date, event_type) %>%
  summarise(count = n()) %>%
  ungroup() %>%

  # Using lubridate functions below
  mutate(year = year(date),
         month = month(date),
         day = day(date))


# Plotting it
ggplot() +
  geom_bar(data = subset(acled_new, year %in% seq(2013, 2018)),
           aes(x = date,
               y = count,
               fill = event_type),
           stat = "identity") +
  facet_wrap(~ year, scales = "free_x", ncol = 1) +
  theme_light()
```


Does Ramadan have an effect on violence in Pakistan? We retrieve Ramadan dates from  http://www.ichild.co.uk/p/when-is-ramadan-2013-2014-2015-2016-2017-2018-2019-2020-2021-2022 and add a layer showing the month of Ramadan in the plot below. 
```{r, warning = F, message = F, fig.height = 10, fig.width = 8}
# Intervals
ram <- c(interval(ymd("2013-07-09"), ymd("2013-08-07")),
         interval(ymd("2014-06-28"), ymd("2014-07-27")),
         interval(ymd("2015-06-18"), ymd("2015-07-17")),
         interval(ymd("2016-06-07"), ymd("2016-07-06")),
         interval(ymd("2017-05-27"), ymd("2017-06-25")),
         interval(ymd("2018-05-16"), ymd("2018-06-14")))

int_start(ram)

df_ramadan <- data.frame(start = int_start(ram),
                         end = int_end(ram)) %>%
  mutate(year = year(start))


ggplot() +
  geom_bar(data = subset(acled_new, year %in% seq(2013, 2018)),
           aes(x = date,
               y = count,
               fill = event_type),
           stat = "identity") +
  facet_wrap(~ year, scales = "free_x", ncol = 1) +
  geom_rect(data = df_ramadan,
            aes(xmin = as.Date(start),
                xmax = as.Date(end),
                ymin = -Inf,
                ymax = Inf),
            alpha = 0.1,
            fill = "#BF8830") +
  theme_light() +
  scale_fill_manual(values = c("Battles" = "#071D70",
                               "Violence against civilians" = "#FF9E00")) +
  labs(title = "Violence in Pakistan",
       x = "",
       y = "Number of events")
```

# Visualizing regression results
Today, we will be working with the `politicalInformation` dataset from the `pscl` ("Political Science Computational Laboratory") package.
```{r}
library(tidyverse)

# install.packages("pscl")
library(pscl)
head(pscl::politicalInformation)
```

We make a copy the original data set to an object called `dat` and create a new variable that codes an above average knowledge rating for the factor levels `Fairly High` and `Very High`. We also recode a number the `collegeDegree` and `female` factor variables, because the "Yes" and "No" values create issues when extracting names from the model summary object later on).
```{r}
dat <- pscl::politicalInformation %>%
  
  # Note: below we could also use case_when() to code the aboveav variable
  mutate(aboveav = ifelse(y %in% c("Fairly High", "Very High"), 1, 0),
         collegeDegree = case_when(
           collegeDegree == "Yes" ~ 1, 
           T ~ 0
         ),
         female = case_when(
           female == "Yes" ~ 1, 
           T ~ 0
         ))
```

First, lets plot the outcome variable by age. Unfortunately, we cannot see much because of overplotting.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(dat,
       aes(x = age,
           y = aboveav)) +
  geom_point()
```



The graph suffers from overplotting. We can tweak the opacity and size of the points and use jitter to reduce its extent. Below, we also add a smoothing line to show the trend in the graph.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4, echo = F}
ggplot(dat,
       aes(x = age,
           y = aboveav)) +
  geom_point(position = position_jitter(height = 0.1),
             alpha = 0.2,
             size = 0.6) +
  theme_light() +
  geom_smooth() +
  labs(x = "Age",
       y = "Above average political knowledge")
```

Does the effect vary by gender? The plot below shows that female respondents have a lower probability of being classified as having above average political knowledge across all ages.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
names(dat)
ggplot(dat,
       aes(x = age,
           y = aboveav,
           color = factor(female))) +
  geom_point(position = position_jitter(height = 0.1),
             alpha = 0.2,
             size = 0.6) +
  theme_light() +
  geom_smooth() +
  labs(x = "Age",
       y = "Above average political knowledge")
```



Lets run a regression (logit model) of the probability of the interviewers rating a person as above average on political knowledge.
```{r}
mod1 <- glm(aboveav ~ collegeDegree + female + length + age,
           data = dat,
           family = binomial(link = "logit"))
summary(mod1)
```
 
## Coefficient plot
There are a number of packages that offer off-the-shelf solutions to plotting coefficient plots for regression outcomes. In this workshop, we will create a coefficient plot manually. This will allow you to create coefficient plots for models that are not supported by existing packages.
 
Below, we extract properties of interest from the `mod1` object.
```{r}
str(summary(mod1)$coefficients)
dimnames(summary(mod1)$coefficients)

# Note that dimnames() returns a list object, not a vector
df_mod1 <- data.frame(vars = dimnames(summary(mod1)$coefficients)[[1]],
                      coef = summary(mod1)$coefficients[,1],
                      se = summary(mod1)$coefficients[,2]) %>%
  
  # Computing CIs
  mutate(cilo_95 = coef - 1.96*se,
         cihi_95 = coef + 1.96*se,
         cilo_99 = coef - 2.56*se,
         cihi_99 = coef + 2.56*se) 
```

We can graph the coefficient plot using the `geom_point()` aesthetic for the coefficient and the `geom_linerange()` aesthetic for the 95% confidence intervals.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 4}
ggplot(df_mod1,
       aes(x = vars,
           y = coef)) +
  geom_point() +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95))
```

Let's add a thinner line for the 99% confidence confidence interval.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 4}
ggplot(df_mod1,
       aes(x = vars,
           y = coef)) +
  geom_point() +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95),
                 size = 1) +
  geom_linerange(aes(ymin = cilo_99, ymax = cihi_99),
                 size = 0.5)
```

Finally, we flip the axes and order the coefficients based on their size to clean up the plot. We also add a line at zero to illustrate which coefficients are statistically significantly different from zero. Note that I add the zero line before the `geom_point()` aesthetic so it is in the background.
```{r, warning = F, message = F, fig.height = 3, fig.width = 3}
ggplot(df_mod1,
       aes(x = reorder(vars, coef),
           y = coef)) +
  geom_hline(yintercept = 0, alpha = 0.8, linetype = "dashed") +
  geom_point() +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95),
                 size = 1) +
  geom_linerange(aes(ymin = cilo_99, ymax = cihi_99),
                 size = 0.5) +
  coord_flip() +
  theme_light()
```

Suppose, we estimated another model that incorporates a quadratic term for age (since we saw from the earlier plot that age appears to have a curvilinear effect on the probability of being classified as above average on political knowledge). We can add these estimates to the data frame of regression results and plot them on the same coefficient plot to compare the results.

Below, I estimate a new model, `mod2` that includes the squared `age` variable. Note that I add an indicator for the model number `modnum` that we will later use to visually distinguish the results from both models.
```{r, warning = F, message = F, fig.height = 3, fig.width = 3}
dat <- dat %>%
  mutate(age2 = age^2)
mod2 <- glm(aboveav ~ collegeDegree + female + length + age + age2,
           data = dat,
           family = binomial(link = "logit"))
summary(mod2)

# Extracting the estimates
df_mod2 <- data.frame(vars = dimnames(summary(mod2)$coefficients)[[1]],
                      coef = summary(mod2)$coefficients[,1],
                      se = summary(mod2)$coefficients[,2]) %>%
  
  # Computing CIs
  mutate(cilo_95 = coef - 1.96*se,
         cihi_95 = coef + 1.96*se,
         cilo_99 = coef - 2.56*se,
         cihi_99 = coef + 2.56*se) %>%
  
  mutate(modnum = 2)
```

Below, I add a `modnum` indicator to the `mod1` data frame and create a joint dataframe using the `bind_rows()` function (similar to `rbind`). Note, that there are more efficient ways to run and combine the results of multiple regression models in a single data frame using loops and lists.
```{r, warning = F, message = F, fig.height = 3, fig.width = 3}
df_all <- df_mod1 %>%
  mutate(modnum = 1) %>%
  bind_rows(df_mod2)
```

We pass the `modnum` variable to both, the shape and the color parameter. We use `position = position_dodge()` to separate the lines and points for the two models. `ggplot2` recognizes the `modnum` variable as continuous and therefore does not want to map it to the shape parameter. We can turn `modnum` in a `factor` variable that can be mapped to a shape inside `aes()`. The graph shows that with the exception of the intercept, the model results are not altered much by the inclusion of the quadratic term.
```{r, warning = F, message = F, fig.height = 3, fig.width = 5}
ggplot(df_all,
       aes(x = reorder(vars, coef),
           y = coef,
           color = factor(modnum),
           shape = factor(modnum))) +
  geom_hline(yintercept = 0, alpha = 0.8, linetype = "dashed") +
  geom_point(position = position_dodge(width = 0.3)) +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95),
                 size = 1,
                 position = position_dodge(width = 0.3)) +
  geom_linerange(aes(ymin = cilo_99, ymax = cihi_99),
                 size = 0.3,
                 position = position_dodge(width = 0.3)) +
  coord_flip() +
  theme_light()
```


## Predictive probabilities plot
Logit coefficients are themselves not very informative about effect size, in particular with regard to comparing the relative effect size of each coefficient. In addition, the explanatory variables are on different scales, which makes the effect sizes difficult to compare, both in numerical terms as well as in the coefficient plot above. We can illustrate the size of the effects on the predicted probability of being classified as having above average political knowledge by the interviewers, upon plotting the effect over the entire range of explanatory variables and holding all other variables at their mean (or a baseline value for dichotomous variables).
```{r}
# Male, no college degree
scen_male <- expand.grid(collegeDegree = 0,
                         female = 0,
                         length = mean(dat$length, na.rm = T),
                         age = seq(min(dat$age, na.rm = T), max(dat$age, na.rm = T), 1)) %>%
  mutate(age2 = age^2)

# Below, get estimates on link scale, transform to predicted probabilities
# See https://stats.idre.ucla.edu/r/dae/logit-regression/
df_male <- cbind(scen_male, 
                 predict(mod2, newdata = scen_male, type = "link", se = TRUE)) %>%
  mutate(predProb = plogis(fit),
         cilo = plogis(fit - (1.96 * se.fit)),
         cihi = plogis(fit + (1.96 * se.fit))) 
```

Below, we use `geom_ribbon()` to graph the confidence interval around the age estimate. Note that we could use `geom_linerange()` instead.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(df_male, 
       aes(x = age,
           y = predProb)) +
  geom_line() +
  geom_ribbon(aes(ymin = cilo, ymax = cihi),
              alpha = 0.3)
```

**Exercise** Suppose we wanted to compare the effect of age on recorded political knowledge for male and female respondents. 

Please compute the predicted probability of being classified as having above average political knowledge for female respondents, combine the two data frames into one, and graph the results for both male and female respondents in the same plot. Try to re-create the graph below as closely as possible.
```{r, warning = F, message = F, fig.height = 3, fig.width = 5}
scen_female <- expand.grid(collegeDegree = 0,
                         female = 1,
                         length = mean(dat$length, na.rm = T),
                         age = seq(min(dat$age, na.rm = T), max(dat$age, na.rm = T), 1)) %>%
  mutate(age2 = age^2)

df_both <- cbind(scen_female, 
                 predict(mod2, newdata = scen_female, type = "link", se = TRUE)) %>%
  mutate(predProb = plogis(fit),
         cilo = plogis(fit - (1.96 * se.fit)),
         cihi = plogis(fit + (1.96 * se.fit))) %>%
  bind_rows(df_male)

#plotting effect for male and female respondents
ggplot(df_both, 
       aes(x = age,
           y = predProb,
           color = factor(female),
           fill = factor(female))) +
  geom_line() +
  geom_ribbon(aes(ymin = cilo, ymax = cihi),
              alpha = 0.3,
              color = NA) +
  
  # adjusting the appearance of the plot
  scale_color_manual(values = c("0" = "darkblue", 
                                "1" = "darkorange"),
                     name = "",
                     labels = c("Male", "Female")) +
  scale_fill_manual(values = c("0" = "darkblue", 
                                "1" = "darkorange"),
                     name = "",
                     labels = c("Male", "Female")) +
  theme_light() +
  labs(x = "Age",
       y = "Predicted probability of above average",
       title = "Effect of age and gender on political knowledge") +
  coord_cartesian(ylim = c(0,0.6))
```


# Maps in R
## Using `geom_polygon()`
The `R` programming environment offers many powerful tools for the visualization and analysis of spatial data. In this workshop, we focus on the visualization of data using maps. There is a myriad of packages for and approaches to creating maps in `R`. We concentrate on visualizing spatial data with the `ggplot2`  package.

In this first example, we will use map data that is part of the `maps` package in `R` and does not require significant preprocessing before plotting. The `maps` package (https://cran.r-project.org/web/packages/maps/index.html) contains data on lines and polygons for a number of geographical units, including but not limited to, countries of the world, a database of large lakes, as well as United States federal states, counties, and cities.

As a first example, we will create a simple map of the continental United States. We draw the data from the `maps` package and plot it using `ggplot2`.
```{r, warning = F, message = F}
# install.packages("maps")
library(maps)
states_map <- map_data("state")
```

Let us look at the structure of the data we drew from `maps`. The data is stored as a data frame and contains observations that are characterized by unique combinations of longitude and latitude values. Each observation has the following attributes: group, order, region, and subregion if applicable. 
```{r, warning = F, message = F}
head(states_map)
```

The `group` and `order` variables in the data set code relational information of the points. For example, there are 49 regions (all states minus Alaska and Hawaii, plus District of Columbia), 63 groups, and a varying number of points (observations) within each group. The observations denote the border points we plotted previously, and the order counter establishes the sequence in which they should be plotted.
```{r, warning = F, message = F}
head(table(states_map$region))
```

We can use `ggplot2`'s `geom_polygon()` function to plot the observations as polygons, rather than points. In order to do that, we need to specify the grouping parameter. If the data wasn't ordered correctly, we could use the `arrange()` function in the `dplyr` package to establish the correct sequencing of points for each polygon (see demonstration below).
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon()


# Demonstration: messing up the order and creating modern art
states_map_unordered <- states_map %>%
  arrange(long)
ggplot(states_map_unordered, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon()
```

We are operating within the normal `ggplot2` environment, so all regular graphing parameters can be used with maps as well.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon(fill = "darkolivegreen", 
               color = "lightgrey", 
               alpha = 0.5) +
  theme_light()
```

The map appears to be a bit "squished". This is because the scaling of the x-axis and y-axis is not based on the scaling of longitude and latitude. We can pass special mapping parameters to `ggplot2` via the `coord_map()` command to achieve the right aspect ratio or use different map projections.
```{r, fig.height = 3, fig.width = 4, warning = F, message = F}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon(fill = "darkcyan", 
               color = "darkblue", 
               alpha = 0.5) +
  theme_minimal() +
  coord_map()
```

```{r, fig.height = 3, fig.width = 4, warning = F, message = F}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon(fill = "darkcyan", 
               color = "darkblue", 
               alpha = 0.5) +
  theme_minimal() +
  coord_map("polyconic")
```

If you need world maps that capture the historic borders of countries, take a look at Nils Weidmann, Doreen Kuse, and Kristian S. Gleditsch's `cshapes()` package.


### Plotting points on a map
`ggplot2` allows for plotting in layers. We can use this feature to add points to our map of the continental US. For ease, we will use the data included in the `maps` package. The `us.cities` database contains information on US cities with a population greater than 40,000 and all state capitals. The database contains information on cities in Hawaii and Alaska as well. We will drop these observations before plotting them on our map of the continental US using the `dplyr` package.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
cities <- us.cities
table(cities$country.etc)

library(dplyr)
cities_sub <- cities %>%
  filter(!(country.etc %in% c("AK", "HI")))
```


We plot the `cities_sub` data as an additional layer on top of the map of the continental US.
```{r, fig.height = 4, fig.width = 5, warning = F, message = F}
ggplot() +
  geom_polygon(data = states_map, 
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "lightgrey", 
               color = "black", 
               size = 0.2, 
               alpha = 0.2) +
  theme_light() +
  coord_map() +
  geom_point(data = cities_sub, 
             aes(x = long, 
                 y = lat),
             alpha = 0.8) +
  labs(title = "Cities with over 40,000 inhabitants")
```

Again, we can use `ggplot2`'s regular graphing options in maps. In the following map we will use different colors for state capitals and denote the population of the cities through the size of the points.
```{r, warning = F, message = F, fig.height = 4, fig.width = 5}
table(cities_sub$capital)
ggplot() +
  geom_polygon(data = states_map, 
               aes(x = long, 
                   y = lat, 
                   group = group),
               fill = "lightgrey", 
               color = "black", 
               size = 0.2, 
               alpha = 0.2) +
  theme_minimal() +
  coord_map() +
  geom_point(data = cities_sub, aes(x = long, 
                                    y = lat, 
                                    color = factor(capital),
                                    size = pop),
             alpha = 0.7) +
  scale_color_manual(values = c("0" = "darkgrey", 
                                "2" = "red"), 
                     labels = c("City over 40,000", 
                                "State Capital"),
                     name = "Type") +
  scale_size_continuous(name = "Population") +
  theme(legend.position = "bottom") +
  labs(title = "Cities with over 40,000 inhabitants")
```

### Adding text
We can also use the names of the observations to label the state capitals.
```{r, warning = F, message = F, fig.height = 4, fig.width = 5}
ggplot() +
  geom_polygon(data = states_map, 
               aes(x = long, 
                   y = lat, 
                   group = group),
               fill = "lightgrey", 
               color = "black", 
               size = 0.2, 
               alpha = 0.2) +
  theme_minimal() +
  coord_map() +
  geom_point(data = cities_sub, aes(x = long, 
                                    y = lat, 
                                    color = factor(capital),
                                    size = pop),
             alpha = 0.6) +
  scale_color_manual(values = c("0" = "darkgrey", 
                                "2" = "red"), 
                     labels = c("City over 40,000", "State Capital"),
                     name = "Type") +
  scale_size_continuous(name = "Population") +
  theme(legend.position = "bottom") +
  geom_text(data = subset(cities_sub, capital == 2),
            aes(x = long, y = lat, label = name),
            size = 2.5) +
  labs(title = "US state capitals and cities with over 40,000 inhabitants",
       x = "",
       y = "")
```

### Subsetting maps
Since the spatial data is stored in a normal database, we can use subsetting to create maps of geographic units contained within a larger spatial database. For example, we could plot a map of Florida using the data from the `maps` package. We can either subset the data before plotting, or use the subset function when specifying the data frame within `ggplot2`.

```{r, fig.height = 3, fig.width = 4, warning = F, message = F}
ggplot(subset(states_map, 
              region == "florida"), 
       aes(x = long, 
           y = lat, 
           group = group)) +
  geom_polygon() +
  coord_map()
```

## `sf` package
`sf` (simple features) is a package for manipulating and analyzing spatial data in `R` (see https://github.com/r-spatial/sf). 

One of the most commonly used sources of shape files is the Global Administrative Areas Database (GADM, http://www.gadm.org) that offers shape files for the administrative boundaries for most countries of the world free of charge. Which administrative boundaries are available varies by country. For example, for the United States, we have shape files at levels 0 (country), 1 (state), and 2 (county). For example, for India, shape files are available for levels 0 (country), 1 (state), 2 (district), and 3 (taluk).

Below, we get data from GADM database for Pakistan via the `raster` package. Data from the GADM by default is stored as a `SpatialPolygonsDataFrame` object. Take a look at the structure of the `SpatialPolygonsDataFrame` below using `View()`.
```{r, fig.height = 4, fig.width = 3}
library(raster)

# Entire country as 1 geom 
pak0 <- getData('GADM', country = 'PAK', level = 0)
# View(pak0)

# Districts
pak3 <- getData('GADM', country = 'PAK', level = 3) 
```

Sometimes plotting maps is a very slow process, especially if many border points are used to plot polygons. Below, we use the `ms_simplify()` function from the `rmapshaper` package to simplify the `SpatialPolygonsDataFrame` (i.e. use fewer points to represent the polygon). We can then transform it to a simple feature (`sf`) object using `st_as_sf()`; using a pipe to connect the operations.
```{r}
library(sf)
library(rmapshaper)

pak3_simple <- ms_simplify(input = pak3, 
                           keep = 0.015) %>%
  st_as_sf()
```

We can plot the `sf` object using `geom_sf()`.
```{r, message = F, fig.height = 4, fig.width = 3}
ggplot() +
  geom_sf(data = pak3_simple)
```


## Adding events to the map
We use the same ACLED data on battles and violence against civilians in Pakistan as in the previous session.  We use the `st_as_sf()` function to turn the `acled` data frame into an `sf` object; specifying `longitude` and `latitude` for the  `coords` parameter. To plot the data, we need to set a coordinate reference system (crs) inside `st_as_sf()`. Here, we use CRS wgs84 (http://download.geonames.org/export/dump/readme.txt). To plot the event locations on the map using `geom_sf()`, the coordinate reference systems of the `geom_sf()` layers have to match.
```{r, warning = F, message = F, fig.height = 4, fig.width = 3}
library(readr) #contains read_csv() function
library(lubridate)
acled <- read_csv("/Users/thereseanders/Documents/UNI/USC/Resources/R/workshop-dataviz-fsu/Day2/1900-01-01-2019-05-03-Pakistan.csv") 
names(acled)

acled_sf <- acled %>%
  
  # using lubidate to re-format the event_date
  mutate(date = dmy(event_date)) %>%
  
  # turning data frame into sf object and setting crs
  st_as_sf(coords = c("longitude", 
                      "latitude"),
           crs = 4326)

# Make sure CRS is the same
st_crs(acled_sf)
st_crs(pak3_simple)
```

We plot the events to the map adding another `geom_sf()` layer. Below, we subset the `acled_sf` dataframe to the year 2015 to reduce the amount of data to plot.
```{r, warning = F, message = F, fig.height = 4, fig.width = 3}
ggplot() +
  geom_sf(data = pak3_simple) +
  geom_sf(data = subset(acled_sf, year == 2015))
```

We can use the `ggplot2` grammar of graphics on spatial data. Below, we plot a separate plot per year and distinguish between battle events and events involving violence against civilians with the `fill` and `shape` aesthetics. Note that the graph takes a moment to render. 
```{r, warning = F, message = F, fig.height = 8, fig.width = 8}
ggplot() +
  geom_sf(data = pak3_simple,
          color = "grey",
          fill = "lightgrey",
          alpha = 0.4) +
  geom_sf(data = subset(acled_sf, year %in% seq(2016, 2018)),
          aes(color = event_type,
              shape = event_type),
          alpha = 0.5) +
  facet_wrap(~ year, nrow = 1) +
  theme_light() +
  coord_sf()
```

## Choropleth maps
Choropleth maps use differences in shading of specific geographic regions to visualize data. 

We can use the `st_join()` function to merge the points and polygons data frame. Here, we implicitly declare `pak3_simple` to be the master data frame, and "add on" `acled_sf`. This preserves the geometry of the polygons and adds the point pattern events data by duplicating the respective polygon information.

We can then compute the number of events per polygon using the `group_by()` and `summarize()` functions.

Note that below, there are a number of polygons that do not experience any events from our ACLED excerpt, which causes the polygon to be dropped. We therefore drop the `geometry` column using `st_set_geometry(NULL)`. We then set up an empty data frame with all possible observations using `expand.grid()` and regular `left_join()` operations (joining a data frame and an `sf` object): a) for the polygon data from `pak3_simple` and b) `df_sum` data frame that contains the summary information.
```{r}
df_sum <- pak3_simple %>%
  st_join(acled_sf) %>%
  group_by(NAME_3, event_type, year) %>%
  summarise(fatal = sum(fatalities, na.rm = T),
            eventcount = n()) %>%
  st_set_geometry(NULL)

df_full <- expand.grid(NAME_3 = unique(pak3_simple$NAME_3),
                       year = unique(acled_sf$year),
                       event_type = c("Battles", "Violence against civilians")) %>%
  left_join(pak3_simple) %>%
  left_join(df_sum)
```

We can now map, for example, the `eventcount` variable to the `fill` aesthetic inside `geom_sf()`. To reduce the number of data plotted, we subset the data to the years 2016, 2017, and 2018, and plot each year in a separate facet.
```{r, warning = F, message = F, fig.height = 4, fig.width = 8}
ggplot() +
  geom_sf(data = subset(df_full, year %in% seq(2016, 2018)),
          aes(fill = eventcount),
          color = "grey") +
  facet_wrap(~ year) +
  scale_fill_gradientn(colors = c("white", "orange", "darkorange", "red", "darkred"),
                       name = "Number of Events",
                       na.value = "white") +
  theme_light()
```

We can further distinguish between the two event types using `facet_grid()`.
```{r, warning = F, message = F, fig.height = 8, fig.width = 8}
ggplot() +
  geom_sf(data = subset(df_full, year %in% seq(2016, 2018)),
          aes(fill = eventcount),
          color = "grey") +
  facet_grid(event_type ~ year) +
  scale_fill_gradientn(colors = c("white", "orange", "darkorange", "red", "darkred"),
                       name = "Number of Events",
                       na.value = "white") +
  theme_light() +
  coord_sf()
```


## Using Stamen map server in `R`
An alternative to shape files is the use of online map data as the basis for the visualization of spatial data. The `ggmap` and `tmaptools` packages allow us to directly query the Google Maps or Stamen Maps servers for a map. Note that due to a recent change in the access policy to Google Maps, you need a google maps API access key to access Google Maps using `ggmap`. In this tutorial, we will use a map from [Stamen](http://maps.stamen.com/#watercolor/12/37.7706/-122.3782).
```{r, message = FALSE, warning = FALSE, fig.height = 4}
# Accessing Stamen map
# https://stackoverflow.com/questions/52704695/is-ggmap-broken-basic-qmap-produces-arguments-imply-differing-number-of-rows/52710855#52710855
library(tmaptools)
library(ggmap)
# default is watercolor
la <- ggmap(get_stamenmap(rbind(as.numeric(paste(geocode_OSM("Los Angeles county")$bbox))), 
                          zoom = 8))
la
```

We can change the appearance of the plot using the `maptype` parameter. To get a list of all types for Stamen maps, see `??get_stamenmap`.
```{r, message = FALSE, warning = FALSE, fig.height = 4}
la_bw <- ggmap(get_stamenmap(rbind(as.numeric(paste(geocode_OSM("Los Angeles county")$bbox))), 
                          zoom = 8,
                          maptype = "toner-lite"))
la_bw
```

### Assessing Los Angeles Air Quality
We can plot additional information on the maps we retrieved from Stamen Maps. As an example, let us consider air quality measurements. As of Spring 2019, the United States Environmental Protection Agency (EPA) still publishes fine-grained geo-coded data on their air quality measurement stations. Here we use a data file that contains 24-hour average fine particulate matter (PM2.5, Federal Reference Method) readings for all US measurement stations in 2018 (Source: file `daily_88101_2018.zip` from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI). Unzip the file and load the csv using `read_csv()` from the `readr` package below.
```{r}
library(readr)
aqi <- read_csv("daily_88101_2018.csv")
names(aqi)
table(aqi$`Sample Duration`)

aqi_la <- aqi %>%
  filter(`Sample Duration` == "24 HOUR") %>%
  filter(`County Name` == "Los Angeles")
```

Below, we compute the median fine particulate matter air quality index for each LA county site in 2018.
```{r}
table(aqi_la$`Local Site Name`)
aqi_la_summary <- aqi_la %>%
  
  # Grouping by Longitude, Latitude, `Local Site Name` to retain variables
  dplyr::group_by(Longitude, Latitude, `Local Site Name`) %>%
  dplyr::summarise(median_aqi = median(AQI, na.rm = T))
```

We then plot the air quality data onto the map of Los Angeles we retrieved from Stamen maps above, letting the color of the points represent the median air quality index for each station. The air quality gets worse as we move closer to Downtown Los Angeles.
```{r, message = FALSE, warning = FALSE, fig.height = 4}
la_bw +
  geom_point(data = aqi_la_summary, 
             aes(x = Longitude, 
                 y = Latitude, 
                 color = median_aqi), 
             size = 10, 
             alpha = 0.7) +
  scale_color_gradient(low = "green", 
                       high = "red", 
                       name = "Median PM2.5") +
  labs(title = "LA County Air Quality Index for\nFine Particulate Matter in 2018") +
  coord_map(ylim = c(33.5, 34.8))
```

## Density Map of Fine Particulate Matter Pollution
According to the EPA, fine particulate matter PM2.5 should not exceed 35 microcrams per cubic meter of air in a 24-hour average (https://www.epa.gov/criteria-air-pollutants/naaqs-table). Rather than looking at the annual median value of the air quality standard to assess the level of air pollution, we could measure the number of times a station reading exceeds this standard. 

To do this for the continental states, we use a subset of our earlier U.S. states map from the `maps` package. Below, we create an `sf` object from the "state" map.
```{r, message = FALSE, warning = FALSE}
us_map <- st_as_sf(map("state", 
                       plot = FALSE, 
                       fill = TRUE))

ggplot(us_map) +
  geom_sf()
```

We then subset the 2018 EPA data to include only cases where the 24-hour average PM2.5 reading exceeded the 35 microcrams per cubic meter of air standard.
```{r, message = FALSE, warning = FALSE}
pm25 <- aqi %>%
  dplyr::filter(`Sample Duration` == "24 HOUR") %>%
  dplyr::filter(`Arithmetic Mean` >= 35,
                !(`State Name` %in% c("Hawaii", "Alaska")))
```

Finally, we plot the density of observations that exceed the daily air quality standard on the map using `ggplot2`'s `stat_density2d()` function. Note that we are not mapping a specific variable, but rather plot the frequency and clustering of observations in our `pm25` subset of all station readings in 2018. This is achieved through the `stat(level)` argument that is passed to the `fill` parameter inside the `stat_density2d()` function.
```{r, message = FALSE, warning = FALSE, fig.height = 4}
ggplot() +
  geom_sf(data = us_map) +
  geom_point(data = pm25, 
                 aes(x = Longitude, 
                     y = Latitude),
             alpha = 0.5,
             color = "red")


ggplot() +
  geom_sf(data = us_map) +
  stat_density2d(data = pm25, 
                 aes(x = Longitude, 
                     y = Latitude,
                     
                     #mapping observations not any specific variable
                     fill = stat(level)),
                 
                 alpha = 0.4,
                 geom = "polygon") +
  scale_fill_gradient(low = "black", high = "red", 
                       name = "Density of daily\nair quality exceeding\nPM2.5 Standard") +
  labs(title = "Fine Particulate Matter Pollution in 2018")
```


# Sources {-}
Bacoffe, A. and Silver, N., 2019. *The 2020 Endorsement Primary*. FiveThirtyEight. Retrieved January 13, 2020.

Pew Research Center. 2015. Building Pew Research Center's American Trends Panel, Technical Report, Washington D.C. Available at http://pewrsr.ch/1Jo4nKE. 

